{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOPP 2020W Exercise 3\n",
    "## Group 10\n",
    "\n",
    "**Question:**\n",
    "\n",
    "How has the use of nuclear energy evolved over time? How well does the use of nuclear energy correlate with changes in carbon emissions? Are there characteristics of a country that correlate with increases or decreases in the use of nuclear energy?\n",
    "\n",
    "**Members:**\n",
    "\n",
    "* Frank Ebel 01429282\n",
    "* Josef Glas 08606876\n",
    "* Felix Korbelius 01526132\n",
    "* Johannes Schabbauer 11776224\n",
    "<br> <span style=\"color:red\">todo_frank Name order?</span>\n",
    "\n",
    "**Work method:**\n",
    "\n",
    "Each person wrote small python scripts for their respective tasks. These scripts were merged in this notebook by Frank and modified if necessary. <br> <span style=\"color:red\">todo_frank Link to GitHub repository?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "# for working with data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# country manipulation\n",
    "import country_converter\n",
    "import pycountry\n",
    "import logging  # todo_frank is it here appropiate?\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding appropiate datasets.\n",
    "\n",
    "We decided that each person should focus on different categories. Each dataset should be sorted by the combination of country and year. To be consistent with country names we decided to use ISO 3166 alpha 3 country codes.  They categories were divided the following way:\n",
    "\n",
    "\n",
    "* Josef: energy consumption and production data\n",
    "* Frank: economical data (GDP, growth, ...)\n",
    "* Felix: ecological data (CO$_2$-emission, pollution, ...)\n",
    "* Johannes: (operating reactors, government, , accidents, <span style=\"color:red\">climate agreements</span> ...)\n",
    "<br> <span style=\"color:red\">todo_frank Name order?</span>\n",
    "\n",
    "**Used datasets**\n",
    "\n",
    "All used datasets are in the folder `./data`.\n",
    "\n",
    "* __[U.S. Energy Information Administration](https://www.eia.gov/international/data/world)__\\\n",
    "`USEIA/`\n",
    "* __[GDP (current USD)](https://data.worldbank.org/indicator/NY.GDP.MKTP.CD)__\\\n",
    "`API_NY.GDP.MKTP.CD_DS2_en_csv_v2_1740389/`\n",
    "* __[GDP growth (annual \\%)](https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG)__\\\n",
    "`API_NY.GDP.MKTP.CD_DS2_en_csv_v2_1740389/`\n",
    "* __[GDP per capita (current USD)](https://data.worldbank.org/indicator/NY.GDP.PCAP.CD)__\\\n",
    "`API_NY.GDP.PCAP.CD_DS2_en_csv_v2_1740213`\n",
    "* __[GDP per capita growth (annual \\%)](https://data.worldbank.org/indicator/NY.GDP.PCAP.KD.ZG)__\\\n",
    "`API_NY.GDP.PCAP.KD.ZG_DS2_en_csv_v2_1740284/`\n",
    "* __[Adjusted net national income per capita (current USD)](https://data.worldbank.org/indicator/NY.ADJ.NNTY.PC.CD)__\\\n",
    "`API_NY.ADJ.NNTY.PC.CD_DS2_en_csv_v2_1745486`\n",
    "* __[Adjusted net national income per capita (annual \\% growth)](https://data.worldbank.org/indicator/NY.ADJ.NNTY.PC.KD.ZG)__ \\\n",
    "`API_NY.ADJ.NNTY.PC.KD.ZG_DS2_en_csv_v2_1745488/`\n",
    "* __[Data on CO2 and Greenhouse Gas Emissions by Our World in Data](https://github.com/owid/co2-data)__\\\n",
    "`owid-co2-data.csv`\n",
    "* __[Power Reactor Information System](https://pris.iaea.org/PRIS/CountryStatistics/CountryStatisticsLandingPage.aspx)__\\\n",
    "`reactor_numbers_PRIS_IAEA.csv`\n",
    "* __[Nuclear Wareheads per country](https://data.world/datagraver/nuclear-warheads-per-country)__\\\n",
    "`nuclear_warheads_1945_2016.csv`\n",
    "* __[Gross domestic expenditure on R&D (GERD), GERD as a percentage of GDP](http://data.uis.unesco.org/Index.aspx?DataSetCode=SCN_DS#)__\\\n",
    "`SCN_DS_16122020083400698.csv`\n",
    "* __[Nuclear Power Accidents (Deaths and Costs)](https://data.world/rebeccaclay/nuclear-power-accidents)__\\\n",
    "`C_id_35_NuclearPowerAccidents2016.csv`\n",
    "* __[The Global State of Democracy Indices](https://www.idea.int/gsod-indices/dataset-resources)__\\\n",
    "`gsodi_pv_4.csv`\n",
    "\n",
    "\n",
    "<br> <span style=\"color:red\">todo_frank datasets order?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading by Josef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize CountryConverter as cc and disable warnings\n",
    "country_converter.logging.getLogger().setLevel(logging.CRITICAL)\n",
    "cc = country_converter.CountryConverter()\n",
    "\n",
    "def load_useia_data():\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    ### CONSUMPTION\n",
    "    target = './data/USEIA/USEIA_CONSUMPTION_1980-2018.csv'\n",
    "\n",
    "    consumption = pd.read_csv(target, sep=\",\", decimal=\".\", header=0, skiprows=1, na_values='--')\n",
    "\n",
    "    # rename columns\n",
    "    consumption = consumption.rename(columns={'Unnamed: 1': 'text'})\n",
    "    consumption.insert(loc=0, column='country', value=0)\n",
    "    consumption.insert(loc=0, column='check', value='X')\n",
    "\n",
    "    consumption['country'] = consumption['API'].str[-10:-7]\n",
    "    #consumption = map_historic_countries(consumption)\n",
    "    consumption['check'] = cc.convert(names=consumption['country'].to_list(), to='ISO3')\n",
    "\n",
    "    consumption = consumption.sort_values(by=['country'])\n",
    "\n",
    "    # data cleaning\n",
    "\n",
    "    # remove rows where country code check failed\n",
    "    raw = consumption[consumption['check'] != 'not found']\n",
    "\n",
    "    raw = raw.reset_index(drop=True)\n",
    "\n",
    "    consumption = pd.DataFrame(columns=['year', 'country', 'cons_btu', 'coal_cons_btu', 'gas_cons_btu', \\\n",
    "                                        'oil_cons_btu', 'nuclear_cons_btu', 'renewables_cons_btu'])\n",
    "\n",
    "    countries = pd.DataFrame(raw['country'])\n",
    "    countries.sort_values(by=['country'])\n",
    "    countries.drop_duplicates(inplace=True)\n",
    "    countries = countries.reset_index(drop=True)\n",
    "\n",
    "    counter = 0\n",
    "    for idx1, c in countries.iterrows():\n",
    "\n",
    "        temp = raw[raw['country'] == c.iloc[0]]\n",
    "\n",
    "        j = 4\n",
    "        for i in range(1980, 2019):\n",
    "\n",
    "            v_cons = 0\n",
    "            v_coal = 0\n",
    "            v_gas = 0\n",
    "            v_oil = 0\n",
    "            v_nuclear = 0\n",
    "            v_renewables = 0\n",
    "\n",
    "            new_row = {'year': i, 'country': c.iloc[0]}\n",
    "            for idx2, row in temp.iterrows():\n",
    "\n",
    "                text = row.iloc[3]\n",
    "\n",
    "                if text.find(\"Consumption\") != -1:\n",
    "                    v_cons = row.iloc[j]\n",
    "                elif text.find(\"Coal\") != -1:\n",
    "                    v_coal = row.iloc[j]\n",
    "                elif text.find(\"Natural gas\") != -1:\n",
    "                    v_gas = row.iloc[j]\n",
    "                elif text.find(\"Petroleum\") != -1:\n",
    "                    v_oil = row.iloc[j]\n",
    "                elif text.find(\"Nuclear (\") != -1:\n",
    "                    v_nuclear = row.iloc[j]\n",
    "                elif text.find(\"Renewables and other\") != -1:\n",
    "                    v_renewables = row.iloc[j]\n",
    "\n",
    "            new_row['cons_btu'] = v_cons\n",
    "            new_row['coal_cons_btu'] = v_coal\n",
    "            new_row['gas_cons_btu'] = v_gas\n",
    "            new_row['oil_cons_btu'] = v_oil\n",
    "            new_row['nuclear_cons_btu'] = v_nuclear\n",
    "            new_row['renewables_cons_btu'] = v_renewables\n",
    "\n",
    "            consumption.loc[counter] = new_row\n",
    "            counter += 1\n",
    "            j += 1\n",
    "\n",
    "    ### PRODUCTION\n",
    "    target = './data/USEIA/USEIA_PRODUCTION_1980-2018.csv'\n",
    "\n",
    "    production = pd.read_csv(target, sep=\",\", decimal=\".\", header=0, skiprows=1, na_values='--')\n",
    "\n",
    "    # rename columns\n",
    "    production = production.rename(columns={'Unnamed: 1': 'text'})\n",
    "    production.insert(loc=0, column='country', value=0)\n",
    "    production.insert(loc=0, column='check', value='X')\n",
    "\n",
    "    production['country'] = production['API'].str[-10:-7]\n",
    "    #production = map_historic_countries(production)\n",
    "    production['check'] = cc.convert(names=production['country'].to_list(), to='ISO3')\n",
    "\n",
    "    production.sort_values('country')\n",
    "\n",
    "    # data cleaning\n",
    "\n",
    "    # remove rows where country code check failed\n",
    "    raw = production[production['check'] != 'not found']\n",
    "\n",
    "    raw = raw.reset_index(drop=True)\n",
    "\n",
    "    production = pd.DataFrame(columns=['year', 'country', 'prod_btu', 'coal_prod_btu', 'gas_prod_btu', \\\n",
    "                                       'oil_prod_btu', 'nuclear_prod_btu', 'renewables_prod_btu'])\n",
    "\n",
    "    countries = pd.DataFrame(raw['country'])\n",
    "    countries.sort_values(by=['country'])\n",
    "    countries.drop_duplicates(inplace=True)\n",
    "    countries = countries.reset_index(drop=True)\n",
    "\n",
    "    counter = 0\n",
    "    for idx3, c in countries.iterrows():\n",
    "\n",
    "        temp = raw[raw['country'] == c.iloc[0]]\n",
    "\n",
    "        j = 4\n",
    "        for i in range(1980, 2019):\n",
    "\n",
    "            v_prod = 0\n",
    "            v_coal = 0\n",
    "            v_gas = 0\n",
    "            v_oil = 0\n",
    "            v_nuclear = 0\n",
    "            v_renewables = 0\n",
    "\n",
    "            new_row = {'year': i, 'country': c.iloc[0]}\n",
    "            for idx4, row in temp.iterrows():\n",
    "\n",
    "                text = row.iloc[3]\n",
    "\n",
    "                if text.find(\"Production\") != -1:\n",
    "                    v_prod = row.iloc[j]\n",
    "                elif text.find(\"Coal\") != -1:\n",
    "                    v_coal = row.iloc[j]\n",
    "                elif text.find(\"Natural gas\") != -1:\n",
    "                    v_gas = row.iloc[j]\n",
    "                elif text.find(\"Petroleum\") != -1:\n",
    "                    v_oil = row.iloc[j]\n",
    "                elif text.find(\"Nuclear (\") != -1:\n",
    "                    v_nuclear = row.iloc[j]\n",
    "                elif text.find(\"Renewables and other\") != -1:\n",
    "                    v_renewables = row.iloc[j]\n",
    "\n",
    "            new_row['prod_btu'] = v_prod\n",
    "            new_row['coal_prod_btu'] = v_coal\n",
    "            new_row['gas_prod_btu'] = v_gas\n",
    "            new_row['oil_prod_btu'] = v_oil\n",
    "            new_row['nuclear_prod_btu'] = v_nuclear\n",
    "            new_row['renewables_prod_btu'] = v_renewables\n",
    "\n",
    "            production.loc[counter] = new_row\n",
    "            counter += 1\n",
    "            j += 1\n",
    "\n",
    "    data = pd.merge(consumption, production, how=\"outer\", on=['year', 'country'])\n",
    "\n",
    "    data['year'] = data['year'].astype('int64')\n",
    "    data['nuclear_prod_btu'] = data['nuclear_prod_btu'].astype('float64')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading by Frank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_economical_data():\n",
    "    \"\"\"Load economical data into dataframe and return it.\n",
    "\n",
    "    Common data structure:\n",
    "    0: year\n",
    "    1: country code\n",
    "    3..: features\"\"\"\n",
    "\n",
    "    def get_df(filepath):\n",
    "        df = pd.read_csv(filepath, sep=',', skip_blank_lines=True, header=2)\n",
    "        df.drop(columns_drop, axis=1, inplace=True)\n",
    "        df.rename(columns={'Country Code': 'country'}, inplace=True)\n",
    "        return df\n",
    "\n",
    "    columns_drop = ['Country Name', 'Indicator Name',  'Indicator Code', 'Unnamed: 65']  # columns to drop\n",
    "    dfs = []  # List of all dataframes.\n",
    "\n",
    "    # load dataframe of GDP\n",
    "    df_GDP = get_df('./data/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_1740389/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_1740389.csv')\n",
    "    # melt and order to get in right format\n",
    "    df_GDP = df_GDP.melt(id_vars=['country'], var_name='year', value_name='GDP')\n",
    "    df_GDP['year'] = df_GDP['year'].astype('int64')\n",
    "    dfs.append(df_GDP)\n",
    "\n",
    "    # load dataframe of GDP growth\n",
    "    df_GDP_growth = get_df('./data/API_NY.GDP.MKTP.KD.ZG_DS2_en_csv_v2_1836177/'\n",
    "                           'API_NY.GDP.MKTP.KD.ZG_DS2_en_csv_v2_1836177.csv')\n",
    "    # melt and order to get in right format\n",
    "    df_GDP_growth = df_GDP_growth.melt(id_vars=['country'], var_name='year', value_name='GDP growth')\n",
    "    df_GDP_growth['year'] = df_GDP_growth['year'].astype('int64')\n",
    "    dfs.append(df_GDP_growth)\n",
    "\n",
    "    # load dataframe of GDP per capita\n",
    "    df_GDP_per_capita = get_df('./data/API_NY.GDP.PCAP.CD_DS2_en_csv_v2_1740213/'\n",
    "                               'API_NY.GDP.PCAP.CD_DS2_en_csv_v2_1740213.csv')\n",
    "    # melt and order to get in right format\n",
    "    df_GDP_per_capita = df_GDP_per_capita.melt(id_vars=['country'], var_name='year', value_name='GDP per capita')\n",
    "    df_GDP_per_capita['year'] = df_GDP_per_capita['year'].astype('int64')\n",
    "    dfs.append(df_GDP_per_capita)\n",
    "\n",
    "    # load dataframe of GDP per capita growth\n",
    "    df_GDP_per_capita_growth = get_df('./data/API_NY.GDP.PCAP.KD.ZG_DS2_en_csv_v2_1740284/'\n",
    "                                      'API_NY.GDP.PCAP.KD.ZG_DS2_en_csv_v2_1740284.csv')\n",
    "    # melt and order to get in right format\n",
    "    df_GDP_per_capita_growth = df_GDP_per_capita_growth.melt(id_vars=['country'], var_name='year',\n",
    "                                                             value_name='GDP per capita growth')\n",
    "    df_GDP_per_capita_growth['year'] = df_GDP_per_capita_growth['year'].astype('int64')\n",
    "    dfs.append(df_GDP_per_capita_growth)\n",
    "\n",
    "    # load dataframe of income per capita\n",
    "    df_income_per_capita = get_df('./data/API_NY.ADJ.NNTY.PC.CD_DS2_en_csv_v2_1745486/'\n",
    "                                  'API_NY.ADJ.NNTY.PC.CD_DS2_en_csv_v2_1745486.csv')\n",
    "    # melt and order to get in right format\n",
    "    df_income_per_capita = df_income_per_capita.melt(id_vars=['country'], var_name='year',\n",
    "                                                     value_name='income per capita')\n",
    "    df_income_per_capita['year'] = df_income_per_capita['year'].astype('int64')\n",
    "    dfs.append(df_income_per_capita)\n",
    "\n",
    "    # load dataframe of income per capita growth\n",
    "    df_income_per_capita_growth = get_df('./data/API_NY.ADJ.NNTY.PC.KD.ZG_DS2_en_csv_v2_1745488/'\n",
    "                                         'API_NY.ADJ.NNTY.PC.KD.ZG_DS2_en_csv_v2_1745488.csv')\n",
    "    # melt and order to get in right format\n",
    "    df_income_per_capita_growth = df_income_per_capita_growth.melt(id_vars=['country'], var_name='year',\n",
    "                                                                   value_name='income per capita growth')\n",
    "    df_income_per_capita_growth['year'] = df_income_per_capita_growth['year'].astype('int64')\n",
    "    dfs.append(df_income_per_capita_growth)\n",
    "\n",
    "    # merge and sort all dataframes\n",
    "    result = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        result = result.merge(df, how='outer', on=['country', 'year'])\n",
    "    result.sort_values(['country', 'year'], inplace=True)\n",
    "    result.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Since there are some aggregated values (e. g. WLD for world) remove all rows which don't have a valid\n",
    "    # ISO 3166 Alpha-3 code.\n",
    "    alpha_3_list = [country.alpha_3 for country in list(pycountry.countries)]  # all valid codes\n",
    "    valid_entry = result['country'].isin(alpha_3_list)  # boolean series if each row is valid or not\n",
    "    result = result.loc[valid_entry]\n",
    "    # invalid = set(result.loc[~valid_entry]['country'].tolist())\n",
    "    # print('invalid', invalid)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading by Felix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emission_data():\n",
    "    \"\"\" \n",
    "    Load all emission data files and combine them into a single Pandas DataFrame.\n",
    "    Common data structure: 0-year, 1-country code, 2+-features.\n",
    "    Check for correct typing.\n",
    "\n",
    "    return:\n",
    "    emission_data: data frame containing different emission data per country per year.\n",
    "    \"\"\"\n",
    "\n",
    "    path = './data/owid-co2-data.csv'\n",
    "    df_emission_data = pd.read_csv(path, sep=',')\n",
    "\n",
    "    cols = ['year', 'iso_code']\n",
    "    # Rearrange columns, so that year and country-code (iso-code) are the first two columns.\n",
    "    new_cols = cols + df_emission_data.columns.drop(cols).tolist()\n",
    "    # Drop country column.\n",
    "    df_emission_data = df_emission_data[new_cols].drop(['country'], axis=1)\n",
    "    # Rename iso_code to country and convert to string.\n",
    "    df_emission_data[['iso_code']] = df_emission_data[['iso_code']].astype('string')\n",
    "    df_emission_data = df_emission_data.rename(columns={'iso_code': 'country'})\n",
    "    return df_emission_data\n",
    "\n",
    "\n",
    "def resize_emission(df):\n",
    "    \"\"\" Index dataframe and eliminate non-country specific data.\n",
    "\n",
    "    Attention: When handling NaN values look at the values of a specific column, if there exists a NaN value\n",
    "    above/below a 0 entry, it is highly possible that NaN are truly missing values.\n",
    "\n",
    "    Time-range: 1980-2018\n",
    "\n",
    "    return:\n",
    "    trimmed down and somewhat ordered emission_data.\"\"\"\n",
    "    data_emission_i = df.copy()\n",
    "    # Only keep countries (check len(country_code) == 3) - raw data contains continental data, etc. with a blank\n",
    "    # country code (i.e. length 0).\n",
    "    data_emission_i = data_emission_i[data_emission_i['country'].str.len() == 3]\n",
    "    # Set index on country_code and year (group by country_code).\n",
    "    # data_emission_i = data_emission_i.set_index(['country_code', 'year'])\n",
    "    # Keep most interesting columns:\n",
    "    data_emission_i = data_emission_i.drop(data_emission_i.iloc[:, -5:-2], axis=1)\n",
    "    data_emission_i = data_emission_i.drop(data_emission_i.iloc[:, 16:26], axis=1)  # delete cement,... produc. emission\n",
    "    data_emission_i = data_emission_i.drop(['gdp', 'trade_co2', 'trade_co2_share'], axis=1)\n",
    "    return data_emission_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading by Johannes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize CountryConverter as cc and disable warnings\n",
    "country_converter.logging.getLogger().setLevel(logging.CRITICAL)\n",
    "cc = country_converter.CountryConverter()\n",
    "# dictionary for country replacements (that cannot be read by country_converter)\n",
    "# using current ones for outdated names, e.g. 'USSR' --> 'Russia'\n",
    "_dict_country_repl = {'UK':'United Kingdom', 'USSR':'Russia', 'Soviet Union':'Russia', 'East Germany':'Germany',\n",
    "                      'Illinois':'US', 'Tawian':'Taiwan', 'Yugoslavia':'Serbia', 'Scotland':'United Kingdom'}\n",
    "\n",
    "###################################################################################################\n",
    "def load_political_data():\n",
    "    # read data from diffenernt datasets in the category 'political'\n",
    "        \n",
    "    # nuclear wrheads\n",
    "    # read file and exclude last (empty) line\n",
    "    warheads = pd.read_csv('./data/nuclear_warheads_1945_2016.csv', \n",
    "                           sep=';',thousands='.', decimal=',').iloc[:-1]\n",
    "    warheads['Year'] = warheads['Year'].astype('int')\n",
    "    warheads = warheads.set_index('Year')\n",
    "    # transform datafame from 2D to MultiIndex\n",
    "    warheads = warheads.stack()\n",
    "    warheads = warheads.reset_index()\n",
    "    # set column names and convert country names to ISO3\n",
    "    warheads.columns = ['year', 'country', 'nuclear_warheads']\n",
    "    warheads['country'] = cc.convert(warheads['country'].to_list(), to='ISO3')\n",
    "    warheads = warheads.set_index(['year', 'country'])\n",
    "    \n",
    "    \n",
    "    # research expenditure\n",
    "    research = pd.read_csv('./data/SCN_DS_16122020083400698.csv')\n",
    "    # choose only lines with relatilve expenditure (for all reaseach categories)\n",
    "    research = research.loc[research['Indicator']==\"GERD as a percentage of GDP\"]\n",
    "    # chose relevant columns and rename them\n",
    "    research = research[['Time','Country', 'Value']]\n",
    "    research.columns = ['year', 'country', 'research_%GDP']\n",
    "    # convert to ISO3 and exclude regions (cannot be converted to countrycode)\n",
    "    research['country'] = research['country'].replace({'Oceania (Australia/New Zealand)':'not found'})\n",
    "    research['country'] =  cc.convert(research['country'].to_list(), to='ISO3', not_found='not found')\n",
    "    research = research[research['country'] != 'not found']\n",
    "    research = research.set_index(['year', 'country'])\n",
    "\n",
    "    \n",
    "    # accidents of nuclear power plants\n",
    "    accidents = pd.read_csv('./data/C_id_35_NuclearPowerAccidents2016.csv')\n",
    "    accidents = accidents[['Date', 'Location', 'Cost (millions 2013US$)', 'Fatalities']]\n",
    "    accidents.columns = ['year', 'country', 'accident_cost_MioUSD2013', 'accident_deaths']\n",
    "    # use only year from Date column\n",
    "    accidents['year'] = accidents['year'].str.slice(start=-4).astype('int')\n",
    "    # use last part of Location (usually the country)\n",
    "    accidents['country'] = accidents['country'].str.split(',').str[-1].str.lstrip(' ')\n",
    "    # do some corrections (e.g. old country names or missing ones)\n",
    "    accidents['country'] = accidents['country'].replace(_dict_country_repl)\n",
    "    # conversion to ISO3\n",
    "    accidents['country'] = cc.convert(accidents['country'].to_list(), to='ISO3')\n",
    "    accidents = accidents.set_index(['year', 'country'])\n",
    "    # sum values, if there was more than one accident per year and country\n",
    "    accidents = accidents.sum(level=['year','country'])\n",
    "    \n",
    "    # democarcy indicators\n",
    "    democracy = pd.read_csv('./data/gsodi_pv_4.csv', low_memory=False)\n",
    "    # choose five main categories\n",
    "    democracy = democracy[['ID_year','ID_country_name','C_A1','C_A2','C_A3','C_A4','C_SD51']]\n",
    "    democracy.columns = ['year', 'country', 'representative_government', 'fundamental_rights', \n",
    "                         'checks_on_gouvernment', 'impartial_administration', 'civil_society_participation']\n",
    "    # avoid that 'Southern Africa' is converted to 'ZAF' and count 'East Germany' as 'Germany' \n",
    "    democracy['country'] = democracy['country'].replace(\n",
    "            {'Southern Africa':' ','German Democratic Republic':'Germany'})\n",
    "    democracy['country'] = cc.convert(democracy['country'].to_list(), to='ISO3')\n",
    "    # exclude regions (and east germany)\n",
    "    democracy = democracy[democracy['country'] != 'not found']\n",
    "    democracy = democracy.set_index(['year', 'country'])\n",
    "    # use mean value for duplicate values (EAST and WEST GERMANY)\n",
    "    democracy = democracy.mean(level=['year','country'])\n",
    "\n",
    "    # get number of reactors from seperate function\n",
    "    reactors = load_reactor_numbers()\n",
    "    \n",
    "    # merging and fill some of the missing values\n",
    "    merge = pd.concat(\n",
    "            [reactors,warheads,accidents,research,democracy],\n",
    "            axis=1, join='outer')\n",
    "\n",
    "\n",
    "    #merge.iloc[:,3] = merge.iloc[:,3].unstack().interpolate().stack()\n",
    "    merge.iloc[:,:3] = merge.iloc[:,:3].fillna(value=0)\n",
    "    #merge.iloc[:,4:6] = merge.iloc[:,4:6].fillna(value='None')\n",
    "    \n",
    "    merge = merge.sort_index(level=['country'])\n",
    "    \n",
    "    return merge\n",
    "\n",
    "###################################################################################################\n",
    "def load_reactor_numbers():\n",
    "    # loading number of operational nuclear power plants from IAEA-PRIS database (public version)\n",
    "    \n",
    "    # if data was already loaded from webpages, read directly from saved csv file\n",
    "    if os.path.isfile('./data/reactor_numbers_PRIS_IAEA.csv'):\n",
    "        reactors = pd.read_csv('./data/reactor_numbers_PRIS_IAEA.csv', index_col=[0,1])\n",
    "        return reactors\n",
    "    \n",
    "    # create containers for reactor data per country\n",
    "    startup_dict=dict()\n",
    "    shutdown_dict=dict()\n",
    "\n",
    "    # fetch table for reactors from public webpage\n",
    "    url = 'https://pris.iaea.org/PRIS/CountryStatistics/ReactorDetails.aspx?current='\n",
    "    for num in range(1000): # manual maximal id of reactor\n",
    "        page = requests.get(url+str(num))\n",
    "        if page.status_code < 400: # exclude non-existing IDs\n",
    "            # find country (ISO2) in html and load tables from page\n",
    "            country = re.findall('[\\d\\D]*color=\"DarkGray\"', str(page.content))[0][-26:-24]\n",
    "            country = cc.convert(country, src='ISO2', to='ISO3')\n",
    "            # create dict entries for new countries\n",
    "            if country not in startup_dict.keys():\n",
    "                startup_dict[country] = np.empty(shape=0, dtype='int')\n",
    "                shutdown_dict[country] = np.empty(shape=0, dtype='int')\n",
    "            page_df = pd.read_html(page.content)\n",
    "            if len(page_df) < 3: # exclude reactor if never started\n",
    "                continue\n",
    "            # get year of startup\n",
    "            if page_df[0].iloc[6,1]=='Commercial Operation Date':\n",
    "                # if 'Commercial Operation Date' is not given (NaN), use 'First Grid Connection'\n",
    "                if type(page_df[0].iloc[7,1]) != 'str':\n",
    "                        startup_dict[country] = np.append(startup_dict[country], int(page_df[0].iloc[7,0][-4:]))\n",
    "                else:\n",
    "                    startup_dict[country] = np.append(startup_dict[country], int(page_df[0].iloc[7,1][-4:]))\n",
    "            # get year of reactor shutdown (if given)\n",
    "            if page_df[0].iloc[8,0]=='Permanent Shutdown Date':\n",
    "                shutdown_dict[country] = np.append(shutdown_dict[country], int(page_df[0].iloc[9,0][-4:]))\n",
    "\n",
    "    # calculate operating reactors from startup and shutdown dates\n",
    "    # of each reactor (from dicts) for each country per year\n",
    "    reactors = pd.DataFrame()\n",
    "    for ISO in startup_dict.keys():\n",
    "        if len(startup_dict[ISO])==0:\n",
    "            continue\n",
    "        reactors_country = pd.DataFrame()\n",
    "        reactors_country['year'] = np.arange(startup_dict[ISO].min(),2021)\n",
    "        reactors_country['country'] = np.full(shape=reactors_country.shape[0], fill_value=ISO)\n",
    "        reactors_country['built_reactors'] = np.fromiter(\n",
    "                (startup_dict[ISO][startup_dict[ISO] <= year].size for year in reactors_country['year'] )\n",
    "                ,dtype='int')\n",
    "        reactors_country['shutdown_reactors'] = np.fromiter(\n",
    "                (shutdown_dict[ISO][shutdown_dict[ISO] <= year].size for year in reactors_country['year'] )\n",
    "                ,dtype='int')\n",
    "        reactors_country['operating_reactors'] = reactors_country['built_reactors'] - reactors_country['shutdown_reactors']\n",
    "        reactors = pd.concat([reactors, reactors_country],axis=0)\n",
    "    reactors = reactors.set_index(['year', 'country'])\n",
    "    # save DataFrame to csv-file, to fetch data not everytime\n",
    "    reactors.to_csv('./data/reactor_numbers_PRIS_IAEA.csv')\n",
    "    return reactors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging datasets\n",
    "\n",
    "This was worked on by Frank. The function <span style=\"color:blue\">clean_data_after_merge()</span> was written by Johannes. Since running the next cell takes a lot of time, the merged and cleaned dataframe was written to `./data/data_merged/data.csv`. For exploring the data, loading the csv was much faster than running the code in the next cell each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Roaming\\Python\\Python39\\site-packages\\country_converter\\country_converter.py:578: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  _match_col = self.data[src_format].astype(str).str.replace(\"\\\\..*\", \"\")\n"
     ]
    }
   ],
   "source": [
    "def clean_data_after_merge(df):\n",
    "    \"\"\"Fill some missing data in merged dataframe.\"\"\"\n",
    "\n",
    "    for column in ['built_reactors', 'shutdown_reactors', 'operating_reactors', 'nuclear_warheads']:\n",
    "        df[column].fillna(value=0, inplace=True)\n",
    "    for column in ['accident_cost_MioUSD2013', 'accident_deaths']:\n",
    "        df[column].fillna(value=0, inplace=True)\n",
    "\n",
    "        \n",
    "df_energy = load_useia_data()\n",
    "df_emission = resize_emission(load_emission_data())\n",
    "df_economy = load_economical_data()\n",
    "df_politics = load_political_data()\n",
    "df_politics.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# merge all dataframes:\n",
    "dataframe = df_energy\n",
    "for df in [df_emission, df_economy, df_politics]:\n",
    "    dataframe = dataframe.merge(df, how='left', on=['year', 'country'])\n",
    "\n",
    "# clean up some values\n",
    "clean_data_after_merge(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation and Comments about used datasets:\n",
    "\n",
    "page 5 of exercise3.pdf\n",
    "\n",
    "how to format this part? Everyone their own thoughts? All together?\n",
    "\n",
    "For some information ('built_reactors', 'shutdown_reactors', 'operating_reactors', 'nuclear_recators', 'accident_cost_MioUSD2013', 'accident_deaths') we decided to fill the missing values with the number 0. It was deemed an appropiate solution since there are not a lot of accidents with nuclear power plants (obvious ones like Chernobyl and Fukushima were in the data). \n",
    "\n",
    "suggestion: country conversion (UdSSR), Felix GHG vs CO$_2$ only, Johannes fillna(0) for missing values\n",
    "\n",
    "This part was written by all members.\n",
    "\n",
    "* Josef\n",
    "* Frank \\\n",
    "    Since we decided beforehand what each person had to search, it was much easier for me to find appropiate data. I found data in .csv and .xslx formats. Of these I thought that .csv formats are easier to work in python with. Some web ruslts only offer datasets behind a paywall, which could not be used for this exercise.\n",
    "* Felix\n",
    "* Johannes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
